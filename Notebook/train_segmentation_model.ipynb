{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.image_filenames = sorted(os.listdir(image_dir))  # Ensure order consistency\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.image_filenames[idx])  # Same name for masks\n",
    "\n",
    "        # Open Image & Mask\n",
    "        image = Image.open(image_path).convert(\"RGB\")  \n",
    "        mask = Image.open(mask_path).convert(\"L\")  # Convert mask to grayscale\n",
    "\n",
    "        # Apply transformations (if any)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define correct dataset path\n",
    "DATASET_PATH = \"../artifacts/output_dataset_dir\"  # Change if needed\n",
    "\n",
    "# Correct paths for train, val, and test sets\n",
    "train_images = os.path.join(DATASET_PATH, \"train/images\")\n",
    "train_masks = os.path.join(DATASET_PATH, \"train/masks\")\n",
    "test_images = os.path.join(DATASET_PATH, \"test/images\")\n",
    "test_masks = os.path.join(DATASET_PATH, \"test/masks\")\n",
    "val_images = os.path.join(DATASET_PATH, \"val/images\")\n",
    "val_masks = os.path.join(DATASET_PATH, \"val/masks\")\n",
    "\n",
    "\n",
    "# Define transformations (normalize, resize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to model input size\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "])\n",
    "\n",
    "# Create Dataset Instances\n",
    "train_dataset = TextSegmentationDataset(train_images, train_masks, transform=transform)\n",
    "val_dataset = TextSegmentationDataset(val_images, val_masks, transform=transform)\n",
    "test_dataset = TextSegmentationDataset(test_images, test_masks, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define U-Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder: Use a pre-trained ResNet34 as feature extractor\n",
    "        resnet = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "        self.encoder = nn.Sequential(*list(resnet.children())[:-2])  # Remove FC layer\n",
    "\n",
    "        # Decoder: Upsampling layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, out_channels, kernel_size=1),  # Output layer\n",
    "            nn.Upsample(size=(256, 256), mode=\"bilinear\", align_corners=False)  # Ensure output shape\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# ✅ Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Training Pipeline: \n",
    "Loss Function\n",
    "We'll use Dice Loss, which is great for segmentation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target, smooth=1):\n",
    "        pred = torch.sigmoid(pred)  # Convert logits to probabilities\n",
    "        target = target.float()  # Ensure target is float\n",
    "\n",
    "        # ✅ Ensure shapes match\n",
    "        if pred.shape != target.shape:\n",
    "            target = torch.nn.functional.interpolate(target, size=pred.shape[2:], mode=\"nearest\")\n",
    "\n",
    "        intersection = (pred * target).sum()\n",
    "        return 1 - ((2. * intersection + smooth) / (pred.sum() + target.sum() + smooth))\n",
    "\n",
    "criterion = DiceLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " What this does:\n",
    "✔️ Uses Dice Loss to handle segmentation better than Binary Cross Entropy\n",
    "✔️ Ensures pred and target have the same shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer & Learning Rate Scheduler\n",
    "We'll use Adam optimizer with a learning rate scheduler to adjust learning over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=3, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this does:\n",
    "✔️ Uses Adam optimizer (good for deep networks)\n",
    "✔️ Lowers LR when validation loss stops improving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader (Augmentations & Batch Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 7\n",
      "Number of masks: 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# ✅ Define Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_list = os.listdir(image_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.image_list[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.image_list[idx])\n",
    "\n",
    "        # ✅ Load Image\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "        # ✅ Load Mask (Ensure single channel)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Grayscale\n",
    "\n",
    "        # ✅ Resize to match model input size (256x256)\n",
    "        image = cv2.resize(image, (256, 256))\n",
    "        mask = cv2.resize(mask, (256, 256))\n",
    "\n",
    "        # ✅ Convert to Tensor\n",
    "        image = transforms.ToTensor()(image)\n",
    "        mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# ✅ Set Paths\n",
    "image_dir = \"../artifacts/output_dataset_dir/train/images\"\n",
    "mask_dir = \"../artifacts/output_dataset_dir/train/masks\"\n",
    "\n",
    "# ✅ Check Dataset Size\n",
    "print(\"Number of images:\", len(os.listdir(image_dir)))\n",
    "print(\"Number of masks:\", len(os.listdir(mask_dir)))\n",
    "\n",
    "# ✅ Create DataLoader\n",
    "train_dataset = CustomDataset(image_dir, mask_dir)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model: Define Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for images, masks in dataloader:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "        # ✅ Forward Pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # ✅ Backward Pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this does:\n",
    "✔️ Runs a full training epoch\n",
    "✔️ Uses forward + backward pass\n",
    "✔️ Updates weights using optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train for Multiple Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1689\n",
      "✅ Model Saved!\n",
      "Epoch [2/10], Loss: 0.1433\n",
      "✅ Model Saved!\n",
      "Epoch [3/10], Loss: -0.0644\n",
      "✅ Model Saved!\n",
      "Epoch [4/10], Loss: -0.3976\n",
      "✅ Model Saved!\n",
      "Epoch [5/10], Loss: -0.6002\n",
      "✅ Model Saved!\n",
      "Epoch [6/10], Loss: -0.6499\n",
      "✅ Model Saved!\n",
      "Epoch [7/10], Loss: -0.6744\n",
      "✅ Model Saved!\n",
      "Epoch [8/10], Loss: -0.6794\n",
      "✅ Model Saved!\n",
      "Epoch [9/10], Loss: -0.6845\n",
      "✅ Model Saved!\n",
      "Epoch [10/10], Loss: -0.6890\n",
      "✅ Model Saved!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "    # ✅ Adjust learning rate\n",
    "    scheduler.step(train_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # ✅ Save best model\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(\"✅ Model Saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test IoU: 3.8702\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_iou = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            preds = torch.sigmoid(outputs) > 0.5  # Convert to binary mask\n",
    "            preds = preds.float()  # Convert boolean to float (0s and 1s)\n",
    "            masks = masks.float()\n",
    "\n",
    "            # ✅ IoU Calculation (Fix bitwise operation issue)\n",
    "            intersection = (preds * masks).sum()\n",
    "            union = (preds + masks).clamp(0, 1).sum()  # Ensures values remain 0 or 1\n",
    "            iou = intersection / (union + 1e-6)  # Add small value to prevent division by zero\n",
    "\n",
    "            total_iou += iou.item()\n",
    "            num_samples += 1\n",
    "\n",
    "    return total_iou / num_samples  # Average IoU\n",
    "\n",
    "# ✅ Load the best model correctly\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", weights_only=True))  # Fix warning\n",
    "\n",
    "# ✅ Run Evaluation\n",
    "test_iou = evaluate(model, train_loader, device)\n",
    "print(f\"Test IoU: {test_iou:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
